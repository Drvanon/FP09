\documentclass[10pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{siunitx}
\usepackage{circuitikz}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{float}
\usepackage{graphicx}
\usepackage{wrapfig}
\newcommand{\uuline}[1]{\underline{\underline{#1}}}

\author{\textbf{Students: }Robin Dorstijn, Moritz Epping \\ \textbf{Tutor: }Jakob Kaiser}
\title{Lab Report FP09: Neuromorphic computing}
\date{February 2021}

\begin{document}
\maketitle
\clearpage
\tableofcontents

\clearpage
\begin{abstract}
    In this paper we examine the SPIKEY Chip for neuromorphic computing and
    come to the conclusion that though it is useful for prototyping neural
    networks in a hardware fashion, it is likely not ready for real life
    applications as the electronics are too sensitive to environmental
    factors. Methods for calibrating individual neurons as well as networks
    are suggested.
\end{abstract}

\section{Theoretical Foundations}
\subsection{General background}
\subsubsection{Motivation}
Brains and computers often are compared because of their shared purpose,
decision making, though they have irreconcilable differences in architecture.
Modern computers function on the basis of the van Neumann
architecture\cite{von-Neumann}, which separates the control unit from the memory
and the computation unit. Though it simplifies the structure of the computer and
makes it modular, it does create a clear bottleneck at the communication layer,
commonly known as the von Neumann bottleneck. This creates an energy
inefficiency that is absolutely unacceptable for biological systems like that of
humans, who spend around 20\% of its total energy uptake on the brain\cite{metabolic-rates}.
This most likely has provided significant evolutionary pressure for hominids to
optimize metabolic resource usage\cite{seymour2016fossil}. For this reason a new
architecture for computers has been suggested: neuromorphic\footnote{``Neuro''
as in brain, ``-morphic'' as in having the shape of, forming ``having the shape
of a brain''.} computing.

\subsubsection{Signal processing in the brain}
Neurons are the basic computational components of the
brain, transmitting and morphing signals. Each neuron receives signals from
others at the dendrites, which shift the electric potential at the membrane of
the cell at the site of the synapse\footnote{The connection point of two
dendrites where the axon of one meets the dendrite of the other.}, creating an
ion wave crossing the entire membrane of the neuron, reaching it's axon,
allowing it to pass the signal on to other neurons.

The potential at the neuron membrane as a function of time is plotted in figure
\ref{fig:action-potential}, here it is demonstrated how a potential shift caused
by pre-synaptic\footnote{That is the neuron that passes on a signal to the
post-synaptic one.} neurons initiates a spike in the membrane potential, which
forces a release of ion containing vesicles at the terminal
bulb\footnote{\textbf{Terminal bulb}: the collections of appendages at the end
of the axon that it uses to pass the signal on to connected neurons.},
facilitating the transfer of the signal by changing the relative potential at
the dendrites.

\begin{figure}[ht]
    \centering
    \includegraphics[width=.5\textwidth]{figures/action-potential.png}
    \caption{Formation of an action potential. Source:  \url{https://courses.lumenlearning.com/boundless-biology/chapter/how-neurons-communicate/}}
    \label{fig:action-potential}
\end{figure}

\subsubsection{The LIF model}
In order to emulate the behaviour electronically, a leaky integrate and fire
(LIF) model is used. A single neuron is represented by an electronic circuit
like the one in figure \ref{fig:circuit}. When the chip is not excited, nor
inhibited there is a constant ``leak'' voltage present on the membrane,
representing the biological rest potential of the membrane potential. When the
threshold voltage $V_\text{thres}$, the amplifier functioning as a comparator in
this setting, sends out a $V_{CC}$ signal to a digital unit that processes that
the neuron has fired and is responsible for closing the switch that keeps the
``membrane potential'' $V_m$ at some $V_\text{reset}$ for the refractory
period. This time is meant to mimic the period the voltage at the membrane
increases non-linearly and therefore loses its sensitivity to input. The digital
processing unit is also responsible for sending the signal (regardless of if it
is inhibitory or excitatory) to the neurons that the are connected to one that
just fired.

\begin{figure}[ht]
    \centering
    \begin{circuitikz}[scale = .6, transform shape]
        \draw (0, 0)    to node[midway, above]{$V_m$} (8.5, 0) node[op amp, anchor=+](A1){}; % main line
        \draw (A1.out)  to[short, l=$V_\text{out}$, -o] ++(1, 0);
        \draw (7, 1)    node[left, above] {$V_\text{thres}$} to[short, *-] (A1.-);
        \draw (0, 0)    to[vR, l=$g_l$, *-] (0, -2)
                        to[battery1, l=$E_l$] (0, -3) node[ground] {} (0, -4);
        \draw (2, 0)    to[vR, l=$g_x$, *-] (2, -2)
                        to[battery1, l=$E_x$] (2, -3) node[ground] {} (2, -4);
        \draw (4, 0)    to[vR, l=$g_i$, *-] (4, -2)
                        to[battery1, l=$E_g$] (4, -3) node[ground] {} (4, -4);
        \draw (6, 0)    to[C, l=$C_m$] (6, -2)
                        node[ground] {} (6, -2);
        \draw (8, -3)   to[short, l_=$V_\text{reset}$, o-] (8, -1)
                        to[normal open switch, -*] (8, 0);
        \draw (7.5, -2) node[anchor=north] {$V_\text{reset}$}
                        to[short, o-] (7.5, -1)
                        to[normal open switch, -*] (7.5, 0);
        \draw [densely dashed]
              (3.3, -4) --++(0, 4.6)
                        --++(5, 0)
                        --++(0, -4.6)
                        --++(-5, 0);
    \end{circuitikz}
    \caption{Circuit of a single neuron in a LIF chip.}
    \label{fig:circuit}
\end{figure}


\begin{figure}[ht]
    \centering
    \begin{circuitikz}[scale = .6, transform shape]
        \draw (4, 0)    to node[midway, above]{$V_m$} (8.5, 0) node[op amp, anchor=+](A1){}; % main line
        \draw (A1.out)  to[short, l=$V_\text{out}$, -o] ++(1, 0);
        \draw (7, 1)    node[left, above] {$V_\text{thres}$} to[short, *-] (A1.-);
        \draw (4, 0)    to[vR, l=$g_i$, *-] (4, -2)
                        to[battery1, l=$E_g$] (4, -3) node[ground] {} (4, -4);
        \draw (6, 0)    to[C, l=$C_m$] (6, -2)
                        node[ground] {} (6, -2);
        \draw (8, -3)   to[short, l_=$V_\text{reset}$, o-] (8, -1)
                        to[short, o-] (7.5, -1)
                        to[normal open switch, -*] (7.5, 0);
    \end{circuitikz}
    \caption{Circuit of a single neuron in a LIF chip.}
    \label{fig:circuit-simplified}
\end{figure}


\subsection{Investigating A Single Neuron}

\subsection{Calibrating Neuron Parameters}
In the case of a single unconnected neuron the membrane potential can be
understood with application of the Kirchoff formula to the circuit in figure
\ref{fig:circuit-simplified}.
\[
    C_m \frac{dV_m}{dt} = g_l(E_l - V_m)
\]
Clearly, this is a ordinary first order differential equation and has the
solution
\begin{equation}
    V_m(t) = A \exp(\frac{-g_l}{C_m}t) - E_l
    \label{eq:diff-eq-sol}
\end{equation}
where $A$ is given by the initial condition:
\[
    A = V_m(0) - E_l.
\]

In order to set a neuron to fire at a regular frequency $\tau_m$, we have to take
into account that $V_m$ will be reset to $V_\text{reset}$ when it hits
$V_\text{thres}$. Equation \eqref{eq:diff-eq-sol} can be rephrased in terms of
$t$.
\[
    \tau_m = -\ln(\frac{V_\text{thres} - E_l}{V_\text{reset} - E_l})
    \frac{C_m}{g_l}
\]

The characteristic time constant $\tau_c$ of the system can be found by setting
$V_\text{thres}$ as a function of $E_l$ and $V_\text{reset}$.
\[
    V_\text{thres} = E_l - (E_l - V_\text{reset})\exp(-1)
\]
The resulting $\tau_c$ is then only dependent on $C_m$ and $g_l$:
\begin{align*}
    \tau_m &= -\ln(\frac{E_l - (E_l - (E_l - V_\text{reset})\exp(-1))}{V_\text{reset} - E_l}) \frac{C_m}{g_l}\\
           &= -\ln(\frac{(E_l - V_\text{reset})\exp(-1))}{V_\text{reset} - E_l})\frac{C_m}{g_l} \\
           &= \frac{C_m}{g_l} = \tau_c
\end{align*}
This time constant does not take into account the refractory period which should
adds a fixed contribution. This was mentioned in equation \eqref{eq:tau}, but
becomes relevant again when considering the total time.

This describes a good theory of the neuron, however in reality there are some
serious production artifacts that require calibration.


\subsection{A Single Neuron with Synaptic Input}

\subsection{Short Term Plasticity}

\subsection{Feed-Forward Networks}
One of the simplest neural networks one can consider is the feed-forward
network in which every neuron is arranged so that it passes on the signal it
receives to the next. In nature, a single neuron firing has relatively little
meaning until it is put in a larger context. Most of these contexts can be
simplified by viewing them as `superpositions' feed forward networks. It is
however important to consider that in nature robustness and redundancy is
highly favored in critical systems. The function of a feed-forward network can
be shutdown entirely if one neuron fails for whatever reason. Hence the concept
of populations is introduced, these are groups of neurons that perform the same
logical operations\footnote{That is to say, they are all either inhibitory or
excitatory neurons, they have the same neurons as inputs and the share the same
output neurons.} This is visualised in figure \ref{fig:feed-forward}.

\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
    \includegraphics[width=.5\textwidth]{figures/feedforwardnetwork.png}
    \caption{Feed forward network with length 4 and population size 2. Two input
        neurons are drawn additionally. Exhibitory neurons and connections are drawn
        in red, inhibitory ones in blue.}
    \label{fig:feed-forward}
\end{wrapfigure}

In this experiment, connection weights also become relevant for the first time.
Any synaptic connection is assigned a weight, this weight represents the
membrane potential increase that a signal transfer induces. The importance of
this variable is crucial for networks as it nearly completely describes the
behaviour of a network.


\subsection{Recurrent Networks}

\subsection{A Simple Computation - XOR}
A very common binary operation that is particularly non-linear (and therefore
more complex) is the XOR operation. It can be described with the truth table
\ref{tab:XOR}.
\begin{wraptable}{r}{.5\textwidth}
    \centering
    \begin{tabular}{c | c | c}
        A & B & Q \\ \hline
        0 & 0 & 0 \\
        0 & 1 & 1 \\
        1 & 0 & 1 \\
        1 & 1 & 0
    \end{tabular}
    \caption{Truth table of the XOR gate. For more information see:
    \cite{horowitz_hill_2020}}
    \label{tab:XOR}
\end{wraptable}
A trivial solution for this problem was found and dismissed thanks to its
probable fragility.  Instead the institute provided a different network
architecture that has significant benefits over the suggested one: it is much
more robust to what is called referred to with `width' in section
\ref{sec:feed-forward}. The inhibitory neurons that are operating in a recurrent
fashion prevent leaking voltage from causing the any excitatory neuron in the
network to fire out of line. This is especially important in an XOR gate, as the
chance that noise would `cancel out' is negligible.

\begin{figure}
    \centering
    \includegraphics[width=.5\textwidth]{figures/XOR-used.png}
    \caption{Network that was used to test the ability of neurons to perform
        logical operations. Functions as an XOR gate.}
    \label{fig:XOR-used}
\end{figure}


\section{Experimental Procedure}

\subsection{Investigating A Single Neuron}

\subsection{Calibrating Neuron Parameters}
On the SPIKEY chip 4 neurons were isolated programmatically and given the same
parameters:
\begin{itemize}
    \item $V_\text{reset}$: \SI{-80.0}{\milli\volt}
    \item $V_\text{thresh}$: \SI{-55.0}{\milli\volt}
    \item $E_\text{leak}$: \SI{-50.0}{\milli\volt}
    \item $g_\text{leak}$:  \SI{20.0}{\nano\siemens}
\end{itemize}

This however resulted in wildly differing frequencies. In order to compensate
for this $g_l$ was adjusted individually for all of the membranes so that they
were all correct within their respective standard deviation:
\SI{20.1}{\milli\volt}, \SI{55.0}{\milli\volt}, \SI{60.0}{\milli\volt},
\SI{20.5}{\milli\volt}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=.5\textwidth]{figures/4membranes.png}
    \caption{Biological membrane potential of four membranes with the same
    setting. The red dots indicate when the digital processing units recognizes
    that voltage reaches the threshold. The rates for the signals are
    \SI{1.02(2)e1}{\milli\second}, \SI{2.41(3)e1}{\milli\second},
    \SI{2.36(1)e1}{\milli\second}, \SI{1.15(1)e1}{\milli\second}, in the same
    order that they presented in the image. }
    \label{fig:4membranes}
\end{figure}

The scale of the problem becomes even more clear when considering the that one
half of the SPIKEY chip has a rate distribution like presented in figure
\ref{fig:distribution}, the previous method of trial and error becomes rather
infeasible.

\begin{figure}[ht]
    \centering
    \includegraphics[width=.5\textwidth]{figures/rate-distribution.png}
    \caption{Distribution of firing rate for the same across one side of the
    chip.}
    \label{fig:distribution}
\end{figure}

Instead an algorithm is suggested that should help find a proper calibration for
all neurons that are able to converge on the desired rate.
\begin{enumerate}
    \item Set the $g_l$ to the default value and measure the rate of the neuron.
    \item Set the $g_l$ to double the default value and measure the rate of the
        neuron.
    \item Describe the response of the neuron linearly using the last two
        points and estimate where the desired rate would lie.
    \item Set the $g_l$ to this value and measure the rate, if it is within the
        standard error: success! If not return to step 3.
\end{enumerate}
Sadly we were unable to implement the algorithm, but we encourage the reader
to.


\subsection{A Single Neuron with Synaptic Input}

\subsection{Short Term Plasticity}

\subsection{Feed-Forward Networks}

\subsection{Recurrent Networks}

\subsection{A Simple Computation - XOR}
\section{Experimental Procedure}

\section{Results}

\subsection{Investigating A Single Neuron}

\subsection{Calibrating Neuron Parameters}

\subsection{A Single Neuron with Synaptic Input}

\subsection{Short Term Plasticity}

\subsection{Feed-Forward Networks}
A feed-forward network is described by the population number, the chain length
(see figure \ref{fig:feed-forward}) and the several weights of the following
type connections:
\begin{itemize}
    \item From the initial excitatory neuron to the first excitatory member of the
        chain. (ex0 $\rightarrow$ ex)
    \item From the initial excitatory neuron to the first inhibitory member of the
        chain. (ex0 $\rightarrow$ inh)
    \item From an excitatory neuron in the chain to the next excitatory neuron.
        (ex $\rightarrow$ ex)
    \item From an excitatory neuron in the chain to the next inhibitory neuron.
        (ex $\rightarrow$ inh)
    \item From an inhibitory neuron in the chain to the next excitatory neuron.
        (inh $\rightarrow$ ex)
\end{itemize}
This coincidentally is also the order of sensitivity (top being most sensitive),
particularly when discussing the `width' of the signal. With this is meant the
the amount of consequent signals that follow a single input signal. When having
a large population, a high (inh $\rightarrow$ ex) wheight was capable of
stopping the signal before it would reach the last neuron.

An issue that occurred as the chain length was increased the width of the signal
did as well. A length of 60 was reached creating a width of 10. This worked for
low populations, however with higher populations the membrane voltage was
overall raised to such a degree that the neurons started firing randomly and the
signal was lost. The theoretical maximum amount of neurons that could be used in
a chain is 192, so that for a population of size $n$ the max chain length
$c_\text{max}$ would be $c_\text{max} = 192 / n$.

An interesting phenomena occurs when the network is configured to loop. That is
to say the last neuron excites the first. Sadly it was not possible to set the
signal delay, which would have allowed for more careful investigation, but still
there was a clear effect visible in figure \ref{fig:feed-forward-loop}.

\begin{figure}
    \centering
    \includegraphics[width=.5\textwidth]{figures/feedforward signals loop.png}
    \caption{Feed-forward network configured in a loop. The population is 5, as
        is the chain length. Note that at the end the width starts to increase.
        The top figure shows all signals send by the neurons. The blue area
        represents inhibitory neurons, the red excitatory. The bottom figure
        shows the membrane potential of the first excitatory neuron.}
    \label{fig:feed-forward-loop}
\end{figure}


\subsection{Recurrent Networks}

\subsection{A Simple Computation - XOR}

\section{Experimental Procedure}

\section{Final Conclusions}
Our results clearly show that the SPIKEY chip can be used for experimenting with
neuromorphic computing in the physical world. Most importantly, it could
verified that the elementary building block (LIF-circuit) behaved in a very
favourable way. Different characteristic features in different realizations of
that circuit in the hardware (due to production errors) can be observed but it
was shown to be possible to calibrate each  ``neuron'' in a way that the effect
of production errors is mostly cancelled. The circuit allows short term
plasticity,  a key trait of neurons that allows them to `learn'.  Finally,
leaving the few-neuron regime, it was verified that the hardware can be used to
model circuits with $\approx 200$ neurons, even binary logic was achievable with
the hardware.

However thanks to the temporal instability of the chip it is clearly not
production ready. It is hard to think of a critical application that would have
environmental conditions suited for this chip, that would be simultaneously cost
effective. Calibration would likely be sufficiently time consuming that it would
ruin any cost effectiveness. That is not to say that it is without application
in the world of research, where it can be a useful tool to investigate behaviour
of neural structures by making it behave like physical neurons.

\section{Discussion}
Comparing our quite complicated realization of the XOR-gate to the standard
electrical implementation of the binary gate in digital computing leads us to
one final important insight: When switching computer hardware to neuromorphic
hardware, it does not suffice to merely copy the brain's architecture and
continue to use boolean logic like is done in modern computers. It is far too
fragile, requiring an enormous amount of neurons and an inefficient use of
compute power.

Instead, if neuromorphic computing is to become an integral part of
computational methods, it is key the field adapts to the biological blueprint.
Naturally that would inspire radically new ways of problem solving in signal
processing.

Before that is reached however, a stark increase in the stability of the
hardware is required, as even a jelly fish has over 5000 neurons
\cite{neuronsjellyfish}. This paper has provided a qualitative review of the
chip, however quantitative measurements are still lacking. We hypothesize that
temporal differences in the internal temperature of the chip is likely the main
culprit of this instability, therefore describing both the sensitivity to a
change in temperature and heat generation by the chip during average usage could
be useful indicators of the chips ability to perform consistently.


\end{document}
